import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import tflearn
from tflearn.data_utils import to_categorical
from tflearn.data_utils import VocabularyProcessor

file = open('1data.txt','r',encoding='latin-1').read()
file = (file.split('\n'))
x=[]
y=[]
y1=[]
## spliting our data into categories(y) , sub categories(y1) , features(x)
for i in range(len(file)-1):
    a=(file[i].split(':'))[0]
    y.append(a)
    b = (file[i].split(':'))[1]
    x.append(b.split(' ',1)[1])
    y1.append(b.split(' ',1)[0])

data = (pd.DataFrame([x,y,y1])).T
data['x']=data[0]
data['y']=data[1]
data['y1']=data[2]
del data[0]
del data[1]
del data[2]

uy1 = data['y1'].unique()
uy= data['y'].unique()

vocab_proc = VocabularyProcessor(17)
x = np.array(list(vocab_proc.fit_transform(x)))

vocab_proc2 = VocabularyProcessor(1)
y1 = np.array(list(vocab_proc2.fit_transform(y))) - 1
y1 = to_categorical(y1, nb_classes=len(data['y1'].unique()))
y = np.array(list(vocab_proc2.fit_transform(y))) - 1
y = to_categorical(y, nb_classes=len(data['y'].unique()))
""""""
yfinal = []
for i in range(len(data)):
    a = y[i].reshape((1,6))
    b = y1[i].reshape((47,1))
    dot = np.dot(b,a)
    dot = dot.flatten()
    yfinal.append(dot)


trainx,testx,trainy,testy = train_test_split(x,yfinal,test_size=0.12)


# each input has length 17
net = tflearn.input_data([None, 17])


# The 17 input word integers are then casted out into 128 dimensions each creating a word embedding.
net = tflearn.embedding(net, output_dim=128)


# each input will have size of 17*128 and then we will fed every single 128 sized vectors are into LSTM cells
# dropout used for removing overfitting and reducing size
net = tflearn.lstm(net, 256, dropout=0.5)


### output is then sent to a fully connected layer that will give us our final 282 classes
net = tflearn.fully_connected(net,282,activation='softmax')


##loss here is categorical crossentropy because in seq to seq type model categorical crossentopy gives better result.
# In  these categorical crossentropy we calculate the entropy of every classification split and
# based on maximum information gain, classfication model is choosen
net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,loss='categorical_crossentropy')

## traing our deep neural network
model = tflearn.DNN(net, tensorboard_verbose=3)


model.fit(trainx, trainy, validation_set=(testx, testy), show_metric=True,batch_size=100,n_epoch=5)
