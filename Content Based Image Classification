### content based image classification ###
(comment)                      we are training our model to distinguish between several images e.g. first we are training our 
model with sevreal images of keyboards and mouse and then aur model will later successful able to distingush keyboards and mouse
--------------------------DONE THROUGH 3 WAYS----------------------------------------------------------------------------------
1. PCA reduction
    images pixel are taken as feature but our images; hence we are dealt with large amount of feature then with the help of PCA
 dimensional redcution(covariance/variance matrix) we will reduce of feature list and then could easily be able of build model
2. Tensorflow
    here we are throwing our pixel data as X and label as Y on our neural net model , and at last neural net is giving us result
but due to large no. of feature(neurons) and addtion of extra HIDDEN neuron takes some time and model has to be build on GPU
for this model computations
3. TFlearn
    it is an abstraction framework over tensorflow , in this model we are using   RECURRENT NEURAL NETWORK  ( RNN) ,rnn gives 
 better accuracy to our model.
 ---------------------------------- Code---- -----------------------------------------------------------------------------------
 1 . PCA 
 
 import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from sklearn.decomposition import RandomizedPCA
from bs4 import BeautifulSoup as bs
import re
from PIL import Image

plt.style.use('ggplot')

index=[]
things=["volleyball","football"]
for input in things:
    url = "http://www.bing.com/images/search?q="+input+"&qft=+filterui:color2-bw+filterui:imagesize-large&FORM=R5IR3"
    url = urllib.request.urlopen(url).read()
    soup = bs(url,'html.parser')

    images = soup.find_all('img')
    index= len(images)-1

    for i in range(2,index):
        link = images[i]['src']
        urllib.request.urlretrieve(link , input+str(i)+".jpg")

data=[]
for input in things:
    for i in range(2,index):
        pic = Image.open(str(input)+str(i)+".jpg")
        pic.thumbnail((400, 400), 'NEAREST')
        map = list(pic.getdata())

        map = np.array(map)
        map = map.flatten()
        data.append(map)

data = np.array(data)

pca = RandomizedPCA(n_components=4)
graph = pca.fit_transform(data)
graph = pd.DataFrame(graph)
plt.scatter(graph[0:29][0],graph[0:29][1],c = 'r' , label = things[0])
plt.scatter(graph[29:57][0],graph[29:57][1],c = 'b' , label = things[1])
#plt.show()
print(graph)

-----------------------------------------------------------------------------------------------------------------------------
2.Tensorlfow

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from sklearn.decomposition import RandomizedPCA
from bs4 import BeautifulSoup as bs
import re
from PIL import Image
import tensorflow as tf
import random

plt.style.use('ggplot')


#index=[]
things=["volleyball","football"]
"""
for input in things:
    url = "http://www.bing.com/images/search?q="+input+"&qft=+filterui:color2-bw+filterui:imagesize-large&FORM=R5IR3"
    url = urllib.request.urlopen(url).read()
    soup = bs(url,'html.parser')

    images = soup.find_all('img')
    index= len(images)-1

    for i in range(2,index):
        link = images[i]['src']
        urllib.request.urlretrieve(link , input+str(i)+".jpg")
"""
data=[]
data_y = []
for input in things:
    for i in range(2,30):
        pic = Image.open(str(input)+str(i)+".jpg")
        pic.thumbnail((400, 400), 'NEAREST')
        map = list(pic.getdata())

        map = np.array(map)
        map = map.flatten()
        data.append(map)
        if input==things[0]:
            y = [1,0]
        else:y=[0,1]
        data_y.append(y)
data = np.array(data)
data_y = np.array(data_y)

index = []
for i in range(8):
    range = random.randrange(1,55)
    index.append(range)

test_data = data[index]
test_data_y = data_y[index]

data = np.delete(data,index,axis=0)
data_y = np.delete(data_y,index,axis=0)

print('data formation done!!')


n_nodes_hl1 = 500
n_nodes_hl2 = 500
n_nodes_hl3 = 500

n_classes = 2
batch_size =5

x = tf.placeholder('float', [None, 117300])
y = tf.placeholder('float')

def neural_network_model(data):
    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([117300, n_nodes_hl1])),
                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}

    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),
                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}

    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),
                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}

    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),
                    'biases':tf.Variable(tf.random_normal([n_classes])),}


    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])
    l1 = tf.nn.relu(l1)

    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])
    l2 = tf.nn.relu(l2)

    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])
    l3 = tf.nn.relu(l3)

    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']

    return output

def train_neural_network(x):
    prediction = neural_network_model(x)
   
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )
    
    optimizer = tf.train.AdamOptimizer().minimize(cost)

    epoch_no = 4
    with tf.Session() as sess:
         OLD:
         sess.run(tf.initialize_all_variables())
        

        for epoch in range(epoch_no):
            epoch_loss = 0
            i = 0
            while i < len(data):
                start = i
                end = i + batch_size
                batch_x = np.array(data[start:end])
                batch_y = np.array(data_y[start:end])

                batch, c = sess.run([optimizer, cost], feed_dict={x: batch_x,
                                                                           y: batch_y})
                # print('batch', (end/batch_size),'completed')

                i += batch_size

            print('Epoch', epoch, 'completed out of', epoch_no, 'loss:', epoch_loss)

        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))

        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
        print('Accuracy:', accuracy.eval({x: test_data, y: test_data_y}))

train_neural_network(x)

-----------------------------------------------------------------------------------------------------------------------------
3. TFlearn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from sklearn.decomposition import RandomizedPCA
from bs4 import BeautifulSoup as bs
import re
from PIL import Image
import tensorflow as tf
import tflearn
import random
plt.style.use('ggplot')

#index=[]
things=["volleyball","football"]
data=[]
data_y = []
for input in things:
    for i in range(2,30):
        pic = Image.open(str(input)+str(i)+".jpg")
        pic.thumbnail((400, 400), 'NEAREST')
        map = list(pic.getdata())

        map = np.array(map)
        map = map.flatten()
        data.append(map)
        if input==things[0]:
            y = [1,0]
        else:y=[0,1]
        data_y.append(y)
data = np.array(data)
data_y = np.array(data_y)

pca = RandomizedPCA(n_components=56)
data = pca.fit_transform(data)

index = []
for i in range(8):
    range = random.randrange(1,55)
    index.append(range)

test_data = data[index]
test_data_y = data_y[index]

data = np.delete(data,index,axis=0)
data_y = np.delete(data_y,index,axis=0)


print('data formation done!!')


net = tflearn.input_data([None,56])
net = tflearn.embedding(net, input_dim=10000, output_dim=128)
net = tflearn.lstm(net, 128, dropout=0.8)
net = tflearn.fully_connected(net, 2, activation='softmax')
net = tflearn.regression(net, optimizer='adam', learning_rate=0.01,
                         loss='categorical_crossentropy')

# Training
model = tflearn.DNN(net, tensorboard_verbose=0)
model.fit(data, data_y, validation_set=(test_data, test_data_y), show_metric=True,
          batch_size=10)

